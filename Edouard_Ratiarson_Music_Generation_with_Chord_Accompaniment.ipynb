{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zxcXtgVNzimY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "by Edouard Ratiarson\n",
        "March 21st 2024\n",
        "\n",
        "# Music Generation with Chord Accompaniment\n",
        "\n",
        "This code is an implementation of a music generation system with chord accompaniment.\n",
        "\n",
        "This code demonstrates how to extend a recurrent neural network (RNN) and LSTM-based music generation model to include chord accompaniment that harmonizes with a given melody. We will start by training a model on a collection of piano MIDI files from the Maestro dataset. The model will learn to predict the next note in a sequence, forming a melody. Subsequently, we will algorithmically generate chords that harmonically support the melody, focusing on fitting within the key and enhancing the overall musical piece.\n",
        "\n",
        "1. **Setup and Importing Libraries**: The code begins by installing the required Python libraries such as `pretty_midi`, `tensorflow`, `pandas`, `matplotlib`, and `seaborn`. These libraries are essential for processing MIDI files, building neural networks, data manipulation, and visualization.\n",
        "\n",
        "2. **MIDI File Processing**: The code defines several functions to extract melody and chord information from MIDI files. The `midi_to_melody_and_chords` function processes a MIDI file and separates the melody notes from the chord notes based on their pitch and start times. It returns two pandas DataFrames containing the extracted melody and chord data.\n",
        "\n",
        "3. **Data Preprocessing**: The code includes functions to preprocess the extracted melody and chord data. The `normalize_pitches` function normalizes the pitch values to a range between 0 and 1, which is a common practice in machine learning for music data. The `create_sequences` function generates sequences of chords with a fixed length, suitable for training a sequence prediction model.\n",
        "\n",
        "4. **Dataset Handling**: The code includes a script to automatically download and extract the Maestro v2.0.0 dataset, a collection of piano MIDI files. This dataset is used for training the music generation model.\n",
        "\n",
        "5. **Melody and Chord Extraction**: The code iterates through the Maestro dataset, extracting melody and chord sequences from each MIDI file. It also analyzes the extracted data, calculating the percentage of zero values (silent timesteps) in the chord sequences, which can be helpful for model tuning and training efficiency.\n",
        "\n",
        "6. **One-hot Encoding**: The code defines a function called `to_one_hot` that converts sequences of chord indices into one-hot encoded vectors. This encoding is crucial for categorical classification tasks, allowing the model to treat each chord type as a distinct category.\n",
        "\n",
        "7. **Neural Network Model**: The code sets up a neural network model in TensorFlow for predicting chords based on a given melody. It calculates the number of unique chords in the dataset and defines a sequential model with LSTM layers and a final dense layer with a softmax activation function for chord classification.\n",
        "\n",
        "8. **Model Training**: The code prepares the targets (chords) for training and demonstrates how to train the neural network model using the preprocessed melody and chord sequences.\n",
        "\n",
        "9. **Visualization**: The code includes a section for visualizing the training loss over epochs, which is a useful diagnostic tool for monitoring the model's learning progress.\n",
        "\n",
        "10. **Chord Suggestion and Music Generation**: The code defines functions for suggesting chords for a given melody using the trained model. It also includes functions for generating music by predicting chords based on a seed sequence of melody notes.\n",
        "\n",
        "Overall, this code implements a complete pipeline for music generation with chord accompaniment, including data preprocessing, model training, and inference for chord prediction and music generation."
      ],
      "metadata": {
        "id": "vIXyMID3zjm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Setup\n",
        "\n",
        "- Import necessary libraries and modules.\n",
        "- Define any required constants or global variables for the project.\n"
      ],
      "metadata": {
        "id": "NR9m507gZYDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries\n",
        "!pip install pretty_midi\n",
        "!pip install tensorflow\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "\n",
        "# After installation, you can import the libraries as before\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pretty_midi\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Constants\n",
        "SAMPLING_RATE = 16000  # Sampling rate for audio playback\n",
        "\n",
        "print(\"Setup complete. Ready to process dataset and generate music.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZdtSBzxZtZ1",
        "outputId": "53f65522-d628-498c-974f-97a312299c21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pretty_midi\n",
            "  Downloading pretty_midi-0.2.10.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from pretty_midi) (1.25.2)\n",
            "Collecting mido>=1.1.16 (from pretty_midi)\n",
            "  Downloading mido-1.3.2-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pretty_midi) (1.16.0)\n",
            "Collecting packaging~=23.1 (from mido>=1.1.16->pretty_midi)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pretty_midi\n",
            "  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty_midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592289 sha256=3bf96646348b2668f248a4b757578f4251dd00816d3d8e1b2064f3509d7c03f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/a5/30/7b8b7f58709f5150f67f98fde4b891ebf0be9ef07a8af49f25\n",
            "Successfully built pretty_midi\n",
            "Installing collected packages: packaging, mido, pretty_midi\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed mido-1.3.2 packaging-23.2 pretty_midi-0.2.10\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.2)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.0.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
            "Setup complete. Ready to process dataset and generate music.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block defines the `midi_to_melody_and_chords` function, which processes a MIDI file to separate the melody from the chords. It utilizes the `pretty_midi` library to read and analyze the MIDI file. The function iterates through all the notes in the file, sorting them by their start times to maintain temporal order.\n",
        "\n",
        "For each note, the function checks whether it belongs to the current chord or starts a new one. If a note starts after the current chord's latest note, it concludes the chord, and the highest note of this chord is considered part of the melody, while the rest are considered as chords. This heuristic follows the common musical practice where the melody is often the highest note in the harmonic structure.\n",
        "\n",
        "The extracted melody and chord notes are then converted into pandas DataFrames with columns 'Pitch', 'Start', and 'End', making it easier to manipulate and analyze the musical data further. The function returns these DataFrames, providing a structured representation of the melody and chords extracted from the MIDI file.\n"
      ],
      "metadata": {
        "id": "pMzZNblxa6DE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pretty_midi\n",
        "\n",
        "def midi_to_melody_and_chords(midi_file: str):\n",
        "    \"\"\"Processes a MIDI file to extract melody and chords.\n",
        "\n",
        "    Args:\n",
        "        midi_file: Path to the MIDI file.\n",
        "\n",
        "    Returns:\n",
        "        A tuple of:\n",
        "            * melody_data: A DataFrame with columns 'Pitch', 'Start', 'End'\n",
        "            * chord_data: A DataFrame with columns 'Pitch', 'Start', 'End'\n",
        "    \"\"\"\n",
        "\n",
        "    pm = pretty_midi.PrettyMIDI(midi_file)\n",
        "\n",
        "    all_notes = [note for instrument in pm.instruments for note in instrument.notes]\n",
        "    all_notes.sort(key=lambda note: note.start)\n",
        "\n",
        "    melody_notes = []\n",
        "    chord_notes = []\n",
        "\n",
        "    current_time = 0\n",
        "    current_chord = []\n",
        "\n",
        "    for note in all_notes:\n",
        "        if note.start > current_time:\n",
        "            if current_chord:\n",
        "                # Assume the highest note in the chord is the melody\n",
        "                highest_note = max(current_chord, key=lambda n: n.pitch)\n",
        "                melody_notes.append(highest_note)\n",
        "                chord_notes.extend([n for n in current_chord if n != highest_note])\n",
        "\n",
        "            current_chord = []\n",
        "            current_time = note.end\n",
        "\n",
        "        current_chord.append(note)\n",
        "\n",
        "    # Process any remaining notes in the last chord\n",
        "    if current_chord:\n",
        "        highest_note = max(current_chord, key=lambda n: n.pitch)\n",
        "        melody_notes.append(highest_note)\n",
        "        chord_notes.extend([n for n in current_chord if n != highest_note])\n",
        "\n",
        "    # Convert notes to DataFrames\n",
        "    melody_data = pd.DataFrame([(n.pitch, n.start, n.end) for n in melody_notes], columns=['Pitch', 'Start', 'End'])\n",
        "    chord_data = pd.DataFrame([(n.pitch, n.start, n.end) for n in chord_notes], columns=['Pitch', 'Start', 'End'])\n",
        "\n",
        "    return melody_data, chord_data\n"
      ],
      "metadata": {
        "id": "3VP1rh0KIRXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t9uf0e0Bu0kP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block is designed to extract melody and chord information from a MIDI file. It uses the `pretty_midi` library to parse the MIDI file and extract note events. The `midi_to_melody_and_chords` function processes each note in the MIDI file to separate melody and chord notes based on their pitch and start times.\n",
        "\n",
        "- The melody is determined by selecting the highest note at each point in time, based on the assumption that the melody note is typically the highest note in a chord or musical phrase.\n",
        "- Chord notes are collected separately and are identified by their simultaneous occurrence and harmony with the melody.\n",
        "- The extracted melody and chord notes are then converted into pandas DataFrames, `melody_df` and `chord_df`, for easier data manipulation and analysis. These DataFrames contain columns for pitch, start time, and end time of each note.\n",
        "\n",
        "Additional functions like `get_melody` and `identify_chord` are defined to support the extraction process:\n",
        "- `get_melody` selects the highest note (melody) from a set of chord notes.\n",
        "- `identify_chord` attempts to name chords based on the intervals between the notes in a chord, specifically identifying major and minor triads.\n",
        "\n",
        "This code is fundamental for analyzing the musical structure of the MIDI file and preparing the data for further musicological analysis or for training machine learning models to generate music based on the extracted patterns.\n"
      ],
      "metadata": {
        "id": "Ucgyw74gu3JE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pretty_midi\n",
        "import pandas as pd\n",
        "\n",
        "import pretty_midi\n",
        "\n",
        "\n",
        "max_duration = 5.0\n",
        "\n",
        "def get_melody(chord_notes):\n",
        "    \"\"\"Selects the highest note as the melody.\"\"\"\n",
        "    if chord_notes:\n",
        "        return max(chord_notes)  # Return the highest pitch\n",
        "    else:\n",
        "        return None  # Handle case when no notes are present\n",
        "\n",
        "\n",
        "def midi_to_melody_and_chords(midi_file_path):\n",
        "    \"\"\"Extracts melody and chords from a MIDI file.\n",
        "\n",
        "    Args:\n",
        "        midi_file_path: The path to the MIDI file.\n",
        "\n",
        "    Returns:\n",
        "        A tuple: (melody_df, chord_data)\n",
        "        * melody_df:  A Pandas DataFrame containing the extracted melody notes.\n",
        "        * chord_data: A list of extracted chords.\n",
        "    \"\"\"\n",
        "\n",
        "    pm = pretty_midi.PrettyMIDI(midi_file_path)\n",
        "\n",
        "    all_melodies = []\n",
        "    all_chords = []\n",
        "    melody_notes = []\n",
        "    chord_notes = []\n",
        "\n",
        "    for instrument in pm.instruments:\n",
        "        notes = instrument.notes\n",
        "        notes.sort(key=lambda note: (note.start, -note.pitch))\n",
        "\n",
        "        for note in notes:\n",
        "            is_melody = note == notes[0]\n",
        "            if is_melody:\n",
        "                melody_notes.append((note.pitch, note.start, note.end))\n",
        "            else:\n",
        "                chord_notes.append(note.pitch)\n",
        "\n",
        "    # Convert melody notes to a DataFrame\n",
        "    melody_df = pd.DataFrame(melody_notes, columns=[\"Pitch\", \"Start\", \"End\"])\n",
        "    chord_df = pd.DataFrame(chord_notes, columns=[\"Pitch\"])  # Or adjust the column name\n",
        "\n",
        "    return melody_df, chord_df  # Return both as DataFrames\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def process_midi(file_path):\n",
        "    pm = pretty_midi.PrettyMIDI(file_path)\n",
        "\n",
        "    melody_notes = []\n",
        "    chord_notes = []\n",
        "\n",
        "    for instrument in pm.instruments:\n",
        "        notes = instrument.notes\n",
        "        notes.sort(key=lambda note: (note.start, -note.pitch))  # Sort by start time, then highest pitch\n",
        "\n",
        "        for note in notes:\n",
        "            is_melody = note == notes[0]  # Check if it's the highest note at that given start time\n",
        "            if is_melody:\n",
        "                melody_notes.append((note.pitch, note.start, note.end))\n",
        "            else:\n",
        "                chord_notes.append(note.pitch)\n",
        "\n",
        "    for instrument in pm.instruments:\n",
        "        notes = instrument.notes\n",
        "        notes.sort(key=lambda note: note.start)  # Sort by start time only\n",
        "\n",
        "        time_step = 0.0\n",
        "        while time_step < max_duration:  # Assuming you have a max_duration\n",
        "            chord_notes = [note.pitch for note in notes if note.start <= time_step < note.end]\n",
        "            chord = identify_chord(chord_notes)  # You'll need to implement this\n",
        "            all_chords.append(chord)\n",
        "            time_step += 0.25  # Or some other time increment\n",
        "\n",
        "    new_melody_data = [(note, 0.0, 0.0) for note in melody_data]  # Placeholder for start and end\n",
        "\n",
        "    print(\"Shape of melody_data:\", np.shape(melody_data))\n",
        "    print(\"First five elements of melody_data:\", melody_data[:5])\n",
        "\n",
        "    melody_df = pd.DataFrame(new_melody_data, columns=[\"Pitch\", \"Start\", \"End\"])\n",
        "    return melody_notes, chord_notes\n",
        "\n",
        "\n",
        "def identify_chord(chord_notes):\n",
        "    \"\"\"Identifies basic major and minor triads based on the given notes.\"\"\"\n",
        "\n",
        "    if len(chord_notes) != 3:\n",
        "        return \"Unknown\"  # Only handling triads for now\n",
        "\n",
        "    # Calculate intervals between notes (modulo 12 for octaves)\n",
        "    intervals = [(chord_notes[i] - chord_notes[0]) % 12 for i in range(1, 3)]\n",
        "\n",
        "    # Check for major or minor triad patterns\n",
        "    if intervals == [4, 7]:  # Major triad\n",
        "        root_note = chord_notes[0] % 12  # Determine root note (modulo 12 for octave)\n",
        "        return f\"{note_names[root_note]} Major\"\n",
        "    elif intervals == [3, 7]:  # Minor triad\n",
        "        root_note = chord_notes[0] % 12\n",
        "        return f\"{note_names[root_note]} Minor\"\n",
        "    else:\n",
        "        return \"Unknown\"\n",
        "\n",
        "# Helper: A list to map note numbers (0-11) to note names\n",
        "note_names = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n",
        "\n",
        "\n",
        "# Assuming you have your MIDI file path set correctly\n",
        "basic_midi_file = '/content/basic_midi_file.midi'\n",
        "\n",
        "# Load the MIDI file and extract melody and chords\n",
        "melody_data, chord_data = midi_to_melody_and_chords(basic_midi_file)\n",
        "\n",
        "# Assuming your melody_data is a list of notes represented as numbers or tuples\n",
        "melody_df = pd.DataFrame(melody_data, columns=[\"Pitch\", \"Start\", \"End\"])\n",
        "\n",
        "\n",
        "# Print summaries of extracted melody and chord data\n",
        "print(\"Melody Data:\")\n",
        "print(melody_data.head())  # Prints the first few rows of the melody DataFrame\n",
        "\n",
        "print(\"\\nChord Data:\")\n",
        "print(chord_data.head())  # Prints the first few rows of the chord DataFrame\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "YYz0DX7LIglf",
        "outputId": "a58bbe65-351b-4ad0-9f8c-ad8046806d23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/basic_midi_file.midi'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-06a37d3472b0>\u001b[0m in \u001b[0;36m<cell line: 120>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;31m# Load the MIDI file and extract melody and chords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m \u001b[0mmelody_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchord_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmidi_to_melody_and_chords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasic_midi_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;31m# Assuming your melody_data is a list of notes represented as numbers or tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-06a37d3472b0>\u001b[0m in \u001b[0;36mmidi_to_melody_and_chords\u001b[0;34m(midi_file_path)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \"\"\"\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mpm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretty_midi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrettyMIDI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmidi_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mall_melodies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pretty_midi/pretty_midi.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, midi_file, resolution, initial_tempo)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmidi_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;31m# If a string was given, pass it as the string filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0mmidi_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmido\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMidiFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmidi_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;31m# Otherwise, try passing it in as a file pointer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mido/midifiles/midifiles.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, file, type, ticks_per_beat, charset, debug, clip, tracks)\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/basic_midi_file.midi'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `create_sequences` takes two parameters: `chord_data`, which is a list of chords (each represented as a string), and `seq_length`, which specifies the length of each sequence to be created. The process involves several key steps:\n",
        "\n",
        "- **Unique Chord Identification**: The function first identifies all unique chords in the dataset and sorts them. This step is crucial for creating a consistent mapping from chord strings to integers, which is needed for numerical processing in machine learning models.\n",
        "- **Integer Encoding**: Each chord in the dataset is then mapped to an integer based on the sorted list of unique chords. This integer encoding converts the string representations into a format that can be efficiently processed by neural networks.\n",
        "- **Sequence Creation**: The function iterates through the integer-encoded chord data, creating overlapping subsequences of the specified length (`seq_length`). Each subsequence serves as an input instance for the model, allowing it to learn patterns and dependencies in the chord progressions.\n",
        "\n",
        "The output of the function is a NumPy array of these sequences, ready to be used in the training process of a sequence prediction model, such as an LSTM (Long Short-Term Memory) neural network. This preparation is essential for enabling the model to learn the structure and progression of chords in music.\n"
      ],
      "metadata": {
        "id": "n7cFsOTtu-E1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_sequences(chord_data, seq_length):\n",
        "    \"\"\"Assumes 'chord_data' is a list of chords represented as strings\"\"\"\n",
        "\n",
        "    unique_chords = sorted(list(set(chord_data)))\n",
        "    chord_to_int = dict((c, i) for i, c in enumerate(unique_chords))\n",
        "\n",
        "    # Integer Encoding\n",
        "    int_encoded_data = [chord_to_int[chord] for chord in chord_data]\n",
        "\n",
        "    # Create sequences\n",
        "    sequences = []\n",
        "    for i in range(len(int_encoded_data) - seq_length + 1):\n",
        "        seq = int_encoded_data[i:i + seq_length]\n",
        "        sequences.append(np.array(seq))\n",
        "\n",
        "    return np.array(sequences)\n"
      ],
      "metadata": {
        "id": "1C8yyfw9XpsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `normalize_pitches` function is designed to normalize MIDI pitch values to a range between 0 and 1, a common preprocessing step in machine learning for music data. This normalization helps in standardizing the input data range, making the training process more stable and efficient. The function can handle different data structures:\n",
        "\n",
        "- **DataFrame Handling**: If the input `data` is a pandas DataFrame, the function assumes there is a 'Pitch' column and normalizes these values by dividing each by the maximum MIDI pitch value, 127. This operation is performed in-place, directly modifying the 'Pitch' column of the DataFrame.\n",
        "- **List Handling**: If `data` is a list, the function checks if it contains integers (representing MIDI pitches) or tuples (representing notes with pitch, start, and end times). For a list of integers, each pitch is normalized individually. For a list of tuples, only the pitch element of each tuple is normalized, preserving the timing information.\n",
        "- **Type Checking**: The function includes a type check to ensure that the input `data` is either a list or a DataFrame. If another data type is passed, a `TypeError` is raised to alert the user to the invalid input type.\n",
        "\n",
        "This normalization process is crucial for maintaining consistency across various pieces of music and ensuring that the machine learning model treats all pitch values on the same scale.\n"
      ],
      "metadata": {
        "id": "E-hBO4VWvCdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def normalize_pitches(data):\n",
        "    \"\"\"Normalizes MIDI pitches within a list or DataFrame\n",
        "\n",
        "    Args:\n",
        "        data:  A list of MIDI pitches (or tuples representing notes), or a DataFrame with a 'Pitch' column.\n",
        "\n",
        "    Returns:\n",
        "        The modified data (list or DataFrame) with normalized pitches.\n",
        "    \"\"\"\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        data['Pitch'] /= 127.0\n",
        "    elif isinstance(data, list):\n",
        "        if all(isinstance(item, int) for item in data):  # All elements are integers (pitches)\n",
        "            data = [pitch / 127.0 for pitch in data]\n",
        "        else:  # List of tuples\n",
        "            data = [(pitch / 127.0, start, end) for (pitch, start, end) in data]\n",
        "    else:\n",
        "        raise TypeError(\"Data must be a list or a DataFrame\")\n",
        "\n",
        "    return data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l7w6CBK0Sz70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script is responsible for automatically downloading and extracting the Maestro v2.0.0 dataset, a widely used collection of piano MIDI files for music generation research. The steps executed by this script are:\n",
        "\n",
        "1. **Define Dataset URL**: The URL for the Maestro dataset (`https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip`) is stored in the `url` variable. This dataset is hosted by Google Cloud Storage and is publicly accessible.\n",
        "\n",
        "2. **Download the Dataset**: Using the `requests` library, the script downloads the dataset ZIP file from the specified URL. The `get` method of `requests` fetches the ZIP file, and the content is stored in `zip_content`.\n",
        "\n",
        "3. **Extract the ZIP File**: The `zipfile.ZipFile` function reads the downloaded ZIP file directly from memory using `io.BytesIO(zip_content)`. This method is memory efficient as it avoids saving the ZIP file to disk before extraction. The `extractall` method then extracts the contents of the ZIP file to the specified directory (`/content/maestro-v2.0.0`). This directory should be adjusted based on the desired extraction location in your environment.\n",
        "\n",
        "By automating the download and extraction process, this script streamlines the setup phase for projects utilizing the Maestro dataset, allowing researchers and developers to focus on their analytical and generative tasks without manual data management overhead.\n"
      ],
      "metadata": {
        "id": "wL_-iumqvN4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "# URL of the Maestro dataset\n",
        "url = 'https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip'\n",
        "\n",
        "# Download the ZIP file\n",
        "print(\"Downloading Maestro dataset...\")\n",
        "response = requests.get(url)\n",
        "zip_content = response.content\n",
        "\n",
        "# Extract the ZIP file\n",
        "print(\"Extracting Maestro dataset...\")\n",
        "zip_file = zipfile.ZipFile(io.BytesIO(zip_content))\n",
        "zip_file.extractall(\"/content/maestro-v2.0.0\")  # Adjust this path as necessary\n"
      ],
      "metadata": {
        "id": "6lPrqSFCaSyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script performs the extraction of melody and chord sequences from MIDI files in the Maestro dataset and prepares the data for machine learning model training:\n",
        "\n",
        "1. **MIDI File Processing**: The `process_midi` function reads a MIDI file, extracts the notes, and sorts them by start time and pitch. It then separates the highest note at each time step as the melody and the remaining notes as chords.\n",
        "\n",
        "2. **Dataset Iteration**: The script iterates through the MIDI files in the Maestro dataset directory using `os.walk`. For each file, it calls `process_midi` to extract melody and chord notes, appending them to `all_melodies` and `all_chords` lists, respectively.\n",
        "\n",
        "3. **Early Inspection**: After processing the first directory (or a single file for quick inspection), the script prints the collected melody and chord data to provide an insight into the extracted information.\n",
        "\n",
        "4. **Data Preparation**: Melody sequences are normalized and both melody and chord sequences are transformed into a consistent format suitable for training. The `create_sequences` function generates subsequences of a specified length from the melody data, and `pad_sequences` from Keras standardizes the length of chord sequences.\n",
        "\n",
        "5. **Zero Padding Analysis**: The script calculates and prints the percentage of zero values (silent timesteps) in the chord sequences at each timestep, giving insight into the sparsity of the data, which can be crucial for model tuning and training efficiency.\n",
        "\n",
        "By executing these steps, the script not only processes MIDI files for melody and chord extraction but also prepares the data for machine learning, ensuring it is in a suitable format for sequence prediction tasks common in music generation models.\n"
      ],
      "metadata": {
        "id": "KHzoSiKzvXWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pretty_midi\n",
        "from tqdm import tqdm\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def process_midi(file_path):\n",
        "    \"\"\"Extracts melodies and chords from a MIDI file.\n",
        "\n",
        "    Args:\n",
        "        file_path: The path to the MIDI file.\n",
        "\n",
        "    Returns:\n",
        "        A tuple: (melody_notes, chord_notes)\n",
        "        * melody_notes: A list of extracted melody notes (pitch, start, end)\n",
        "        * chord_notes: A list of extracted chords (lists of pitches)\n",
        "    \"\"\"\n",
        "\n",
        "    pm = pretty_midi.PrettyMIDI(file_path)\n",
        "\n",
        "    melody_notes = []\n",
        "    chord_notes = []\n",
        "\n",
        "    for instrument in pm.instruments:\n",
        "        notes = instrument.notes\n",
        "        notes.sort(key=lambda note: (note.start, -note.pitch))  # Sort by start time, then highest pitch\n",
        "\n",
        "        for note in notes:\n",
        "            is_melody = note == notes[0]\n",
        "            if is_melody:\n",
        "                melody_notes.append((note.pitch, note.start, note.end))\n",
        "            else:\n",
        "                chord_notes.append(note.pitch)\n",
        "\n",
        "    return melody_notes, chord_notes\n",
        "\n",
        "# Assuming the Maestro dataset is extracted at this location\n",
        "dataset_path = '/content/maestro-v2.0.0/maestro-v2.0.0'\n",
        "\n",
        "all_melodies = []\n",
        "all_chords = []\n",
        "\n",
        "# Iterate through the Maestro dataset\n",
        "for root, dirs, files in os.walk(dataset_path):\n",
        "    for file_name in tqdm(files, desc=\"Processing MIDI files\"):\n",
        "        if file_name.endswith(('.midi', '.mid')):\n",
        "            full_path = os.path.join(root, file_name)\n",
        "\n",
        "            try:\n",
        "                melody_notes, chord_notes = process_midi(full_path)\n",
        "                all_melodies.extend(melody_notes)\n",
        "                all_chords.extend(chord_notes)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {full_path}: {e}\")\n",
        "\n",
        "            print(\"Data after processing the first directory:\")\n",
        "            print(\"Melodies:\", all_melodies)  # Inspect the lengths of melody sequences\n",
        "            print(\"Chords:\", all_chords)\n",
        "            break\n",
        "\n",
        "# After the loop completes, you'll have your all_melodies and all_chords lists populated!\n",
        "# ... (Maestro processing loop) ...\n",
        "\n",
        "# Data Preparation\n",
        "melody_sequences = create_sequences(normalize_pitches(all_melodies), seq_length=32)  # Adjust seq_length\n",
        "chord_sequences = pad_sequences(chord_sequences, maxlen=32, padding='post')\n",
        "\n",
        "zero_counts = np.zeros((chord_sequences.shape[1],))\n",
        "for sequence in chord_sequences:\n",
        "    zero_counts += np.sum(np.all(sequence == 0, axis=1))\n",
        "\n",
        "# Calculate percentages\n",
        "zero_percentages = (zero_counts / len(chord_sequences)) * 100\n",
        "\n",
        "print(\"Percentage of zeros per timestep:\")\n",
        "for i, p in enumerate(zero_percentages):\n",
        "    print(f\"Timestep {i+1}: {p:.2f}%\")"
      ],
      "metadata": {
        "id": "YpWkSG49QT06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Extracted Melody from {file_name}: {melody_notes}\")\n",
        "print(f\"Extracted Chords from {file_name}: {chord_notes}\")\n"
      ],
      "metadata": {
        "id": "EJE1P88WQdZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code snippet is focused on finalizing the data preparation phase for a machine learning model in music generation. Here’s what each part accomplishes:\n",
        "\n",
        "1. **Sequence Generation**:\n",
        "   - `create_sequences` function is called for both `all_melodies` and `all_chords`, with a specified `seq_length` of 32. This function generates sequences from the list of notes, where each sequence has a fixed length, ensuring a consistent input size for the model.\n",
        "   - `normalize_pitches` is applied to the melodies and chords to scale the pitch values between 0 and 1, enhancing the model’s training process by providing standardized input.\n",
        "\n",
        "2. **One-hot Encoding**:\n",
        "   - `to_one_hot` function is defined to convert the numerical chord data into one-hot encoded vectors, which are used as labels for the model training. One-hot encoding is crucial for categorical output like chords, as it allows the model to treat each chord type as a distinct category without any implied ordinal relationship.\n",
        "   - Inside the `to_one_hot` function, a zero matrix is created with dimensions corresponding to the sequence length and the number of chord classes. It then marks the appropriate positions in the matrix with 1’s based on the chord indices in each sequence, resulting in a one-hot encoded representation of the chord sequences.\n",
        "\n",
        "3. **Vectorization**:\n",
        "   - The one-hot encoding process transforms the list of chord indices into a 3D NumPy array, where each row corresponds to a timestep in the sequence, and each column represents a chord class. This format is compatible with the input requirements of many neural network architectures, especially those dealing with multi-class classification tasks.\n",
        "\n",
        "This preparation ensures that the data is in the correct format for training a neural network, where the model is expected to predict a chord (as a one-hot encoded vector) for each sequence of melody notes.\n"
      ],
      "metadata": {
        "id": "KnYDzfKewZc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Data Preparation\n",
        "melody_sequences = create_sequences(normalize_pitches(all_melodies), seq_length=32)  # Adjust seq_length\n",
        "chord_sequences = create_sequences(normalize_pitches(all_chords), seq_length=32)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "#def to_one_hot(chord_indices, num_classes):\n",
        "def to_one_hot(chord_sequences, num_classes):\n",
        "    \"\"\"Converts sequences of chord indices into one-hot encoded vectors.\"\"\"\n",
        "    result = []\n",
        "    for sequence in chord_sequences:\n",
        "        one_hot_vectors = np.zeros((len(sequence), num_classes))\n",
        "        one_hot_vectors[np.arange(len(sequence)), sequence] = 1\n",
        "        result.append(one_hot_vectors)\n",
        "    return np.array(result)\n",
        "\n"
      ],
      "metadata": {
        "id": "0ueK-EoxS33b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code snippet is designed to set up a neural network model in TensorFlow for predicting chords in music generation. Here’s how it operates:\n",
        "\n",
        "1. **Unique Chords Calculation**:\n",
        "   - The `flattened_chords` list is created by flattening `chord_sequences`, which are likely sequences of chords used in the training dataset. This step is crucial for understanding the diversity of the chord vocabulary in the data.\n",
        "   - `np.unique` function is applied to find all unique chords in the dataset, and `num_unique_chords` is calculated to determine the number of unique chord categories. This information is critical for the model’s output layer to classify into the correct number of chord categories.\n",
        "\n",
        "2. **Model Definition**:\n",
        "   - A `Sequential` model is defined using TensorFlow’s Keras API, indicating that the layers in the model will be arranged in sequence.\n",
        "   - While the code for adding LSTM layers is not shown (`# ... (your LSTM layers)`), it implies that one or more LSTM layers are expected to precede the final dense layer. LSTM layers are instrumental in capturing the temporal dependencies in the sequence of melody notes.\n",
        "   - The final layer of the model is a `Dense` layer with `num_unique_chords` units, which corresponds to the total number of unique chords found in the dataset. The `softmax` activation function suggests that the model’s output can be interpreted as a probability distribution over all possible chord classes, making it suitable for classification tasks.\n",
        "\n",
        "3. **Compilation and Training** (implied but not shown):\n",
        "   - After defining the model architecture, it would typically be compiled with an optimizer and loss function suitable for classification (e.g., `categorical_crossentropy` for multi-class classification), and then trained on preprocessed and sequence-transformed melody and chord data.\n",
        "\n",
        "This setup ensures that the neural network model is appropriately configured to learn from the sequences of melody data to predict corresponding chords, with an output layer fine-tuned to the specific number of chord variations present in the training dataset.\n"
      ],
      "metadata": {
        "id": "xkMrTYczwcbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# ... (rest of your code)\n",
        "\n",
        "# Calculate the number of unique chords\n",
        "flattened_chords = [chord for chord_sequence in chord_sequences for chord in chord_sequence]\n",
        "unique_chords = np.unique(flattened_chords)\n",
        "num_unique_chords = len(unique_chords)\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.Sequential([\n",
        "    # ... (your LSTM layers)\n",
        "\n",
        "    tf.keras.layers.Dense(num_unique_chords, activation='softmax')\n",
        "])\n",
        "\n",
        "# ... (rest of your code)\n"
      ],
      "metadata": {
        "id": "E9iF9KJaGJTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# ... (Your code for calculating num_unique_chords)\n",
        "\n",
        "print(chord_sequences.shape) # Print the shape here\n",
        "# One-hot encode chord indices\n",
        "chord_sequences = to_one_hot(chord_sequences, num_unique_chords)\n",
        "\n",
        "# Reshape to add features dimension\n",
        "chord_sequences = chord_sequences.reshape(-1, 32, num_unique_chords)\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.LSTM(128, return_sequences=True, input_shape=(32, num_unique_chords)),\n",
        "    tf.keras.layers.LSTM(64),\n",
        "\n",
        "    # Output layer - Adjust based on chord representation\n",
        "    tf.keras.layers.Dense(num_unique_chords, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "9QmDdP_AGiPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section of the notebook demonstrates preparing the chord data for a neural network model and defining the model architecture for chord prediction in music generation. The process unfolds as follows:\n",
        "\n",
        "1. **Inspecting Chord Sequences Shape**:\n",
        "   - `print(chord_sequences.shape)` is used to display the shape of `chord_sequences` before transformation. This is helpful for understanding the data's structure and ensuring that the one-hot encoding process will be applied correctly.\n",
        "\n",
        "2. **One-hot Encoding of Chord Sequences**:\n",
        "   - The `to_one_hot` function is applied to `chord_sequences` with `num_unique_chords` as the total number of chord categories. This converts the chord indices into one-hot encoded vectors, which are suitable for categorical classification tasks in machine learning.\n",
        "\n",
        "3. **Reshaping Data**:\n",
        "   - The one-hot encoded `chord_sequences` are reshaped to fit the input requirements of the LSTM layers in the neural network. The reshape operation ensures that each sequence has a specified length (`32` in this case) and a feature dimension equal to `num_unique_chords`, which represents the one-hot encoded vectors.\n",
        "\n",
        "4. **Neural Network Model Construction**:\n",
        "   - A `Sequential` model is defined using TensorFlow's Keras API, indicating a linear stack of layers.\n",
        "   - Two LSTM layers are specified:\n",
        "        - The first LSTM layer has `128` units and `return_sequences=True`, which ensures that the output is a sequence, feeding into the next LSTM layer.\n",
        "        - The second LSTM layer has `64` units and compacts the sequential information into a single vector.\n",
        "   - The output layer is a `Dense` layer with a size equal to `num_unique_chords` and uses a `softmax` activation function. This setup is ideal for classification tasks where the output represents a probability distribution over the chord categories.\n",
        "\n",
        "5. **Model Compilation**:\n",
        "   - The model is compiled with `categorical_crossentropy` as the loss function, suitable for multi-class classification tasks, and `adam` as the optimizer, known for its efficiency in handling sparse gradients and adaptive learning rates.\n",
        "\n",
        "6. **Model Summary**:\n",
        "   - `model.summary()` provides a textual representation of the neural network architecture, including the configuration of each layer and the total number of parameters that will be trained.\n",
        "\n",
        "In this code block, the preparation of chord data and the definition of the neural network are crucial steps toward training a model capable of predicting chord sequences that harmonize with given melodies.\n"
      ],
      "metadata": {
        "id": "GjSny9hGwudj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# ... (Your code for loading data, creating 'chord_sequences', and calculating 'num_unique_chords')\n",
        "\n",
        "# ... (Your model definition code - which you already have)\n",
        "\n",
        "# Prepare Targets (Assuming predicting the next chord)\n",
        "def prepare_targets(chord_sequences):\n",
        "    targets = []\n",
        "    for seq in chord_sequences:\n",
        "        targets.append(seq[1])  # Take the second chord of each sequence\n",
        "    return np.array(targets)\n",
        "\n",
        "\n",
        "\n",
        "targets = prepare_targets(chord_sequences)\n",
        "\n",
        "print(\"Shape of chord_sequences:\", chord_sequences.shape)\n",
        "print(\"Shape of targets:\", targets.shape)\n",
        "\n",
        "print(\"First 10 sequences:\", chord_sequences[:10])\n",
        "print(\"First 10 targets:\", targets[:10])\n",
        "\n",
        "print(chord_sequences.shape)\n",
        "print(targets.shape)\n",
        "\n",
        "\n",
        "# Model Training\n",
        "model.fit(chord_sequences, targets, epochs=10, batch_size=32)\n"
      ],
      "metadata": {
        "id": "LFu-uJOQJZBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history = model.history  # Assuming you store training results in a 'history' variable\n",
        "epochs = range(1, len(history.history['loss']) + 1)\n",
        "loss_values = history.history['loss']\n",
        "\n",
        "plt.plot(epochs, loss_values, 'o-')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss over Epochs')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tfyyApgVW7fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph shows a plot of training loss over epochs. It illustrates that the loss decreases significantly as the number of epochs increases, indicating that the model is learning and improving its predictions over time. The plot shows a typical learning curve where the rapid decrease in loss suggests quick learning in the initial phase, which then gradually stabilizes as the model begins to converge to a minimum loss value."
      ],
      "metadata": {
        "id": "oQNiAvYCw22M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_melody(midi_data):\n",
        "    melody_instrument = midi_data.instruments[0]  # Assuming melody on the first instrument\n",
        "    melody_notes = []\n",
        "\n",
        "    for note in melody_instrument.notes:\n",
        "        melody_notes.append(note.pitch)\n",
        "\n",
        "    return melody_notes\n",
        "\n",
        "def preprocess_melody(melody_notes):\n",
        "    window_size = 3  # Adjustable\n",
        "    chord_sequences = []\n",
        "\n",
        "    for i in range(len(melody_notes) - window_size + 1):\n",
        "        window = melody_notes[i:i + window_size]\n",
        "        possible_chords = identify_chords(window)  # Function to define\n",
        "        chord_sequences.append(possible_chords)\n",
        "\n",
        "    return chord_sequences\n",
        "\n",
        "\n",
        "def build_chord_dictionary():\n",
        "    chord_dict = {}\n",
        "\n",
        "    # Major Triads\n",
        "    for root in range(12):\n",
        "        chord_dict[(root, root + 4, root + 7)] = f\"{chr(root + 65)}\"  # Example: C Major\n",
        "\n",
        "    # Minor Triads\n",
        "    for root in range(12):\n",
        "        chord_dict[(root, root + 3, root + 7)] = f\"{chr(root + 65)}m\"  # Example: Cm\n",
        "\n",
        "    return chord_dict\n",
        "\n",
        "chord_dictionary = build_chord_dictionary()\n",
        "print(chord_dictionary)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IoLUbfmBYH9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N4tfxz40Ztd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " I've created these functions to process MIDI files into a format that can be used for machine learning. The goal is to teach a model how to harmonize melodies with appropriate chords by recognizing patterns in music theory."
      ],
      "metadata": {
        "id": "dxt3VPQ803qJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def preprocess_melody(melody_notes):\n",
        "    window_size = 3\n",
        "    chord_sequences = []\n",
        "\n",
        "    for i in range(len(melody_notes) - window_size + 1):\n",
        "        window = melody_notes[i:i + window_size]\n",
        "        possible_chords = identify_chords(window)\n",
        "        encoded_chords = [to_one_hot(chord, num_unique_chords) for chord in possible_chords]\n",
        "        chord_sequences.append(encoded_chords)\n",
        "\n",
        "    return chord_sequences\n",
        "\n",
        "def generate_music(model, seed_sequence, num_steps):\n",
        "    sequence = seed_sequence.copy()\n",
        "    current_chord = sequence[-1]\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        melody_window = sequence[-window_size:]  # Extract the melody window\n",
        "        # Concatenate one-hot encoded melody_window and current_chord\n",
        "        model_input = np.concatenate(melody_window + [current_chord]).reshape(1, window_size + 1, -1)\n",
        "        prediction = model.predict(model_input)[0]\n",
        "        new_chord = sample_chord(prediction)\n",
        "        sequence = np.append(sequence, new_chord, axis=0)\n",
        "        current_chord = new_chord\n",
        "\n",
        "    return sequence\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "syFE52n-aVbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `predict_chords_for_melody` function is designed to predict chords for a given melody using a trained neural network model. Here's a summary of what the function does:\n",
        "\n",
        "1. **Input**: The function takes three arguments:\n",
        "   - `model`: The trained neural network model for chord prediction.\n",
        "   - `melody_notes`: A list of numerical values representing the melody notes.\n",
        "   - `window_size`: The size of the melody window used for prediction.\n",
        "\n",
        "2. **Output**: The function returns a list of predicted chords, where each chord is represented as a one-hot encoded vector.\n",
        "\n",
        "3. **Iterating over Melody**: The function iterates over the `melody_notes` list, creating overlapping windows of size `window_size`. For each window, it performs the following steps:\n",
        "   a. Prints the type and content of the window for debugging purposes.\n",
        "   b. Converts the window of melody notes into a one-hot encoded representation using the `to_one_hot` function and the `num_unique_notes` variable (assumed to be defined elsewhere).\n",
        "   c. Expands the dimensions of the one-hot encoded window to match the input shape expected by the model.\n",
        "\n",
        "4. **Prediction**: For each window, the function passes the one-hot encoded melody window to the trained model using `model.predict`. The model then predicts the corresponding chord represented as a one-hot encoded vector.\n",
        "\n",
        "5. **Appending Predictions**: The predicted chord (one-hot encoded vector) for each window is appended to the `predicted_chords` list.\n",
        "\n",
        "6. **Return**: After iterating over all windows, the function returns the `predicted_chords` list, which contains the predicted chords (one-hot encoded) for each window of the melody.\n",
        "\n",
        "In summary, the `predict_chords_for_melody` function takes a trained model, a melody represented as numerical values, and a window size as input. It then iterates over the melody, creating overlapping windows, and uses the trained model to predict the corresponding chords for each window. The predicted chords are represented as one-hot encoded vectors and returned as a list."
      ],
      "metadata": {
        "id": "OQRjcLcM1JN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_chords_for_melody(model, melody_notes, window_size):\n",
        "  \"\"\"\n",
        "  Predicts chords for a given melody using a trained model.\n",
        "\n",
        "  Args:\n",
        "      model: The trained model for chord prediction.\n",
        "      melody_notes: A list of numerical values representing the melody notes.\n",
        "      window_size: The size of the melody window for prediction.\n",
        "\n",
        "  Returns:\n",
        "      A list of predicted chords (one-hot encoded) for each window.\n",
        "  \"\"\"\n",
        "\n",
        "  predicted_chords = []\n",
        "  for i in range(len(melody_notes) - window_size + 1):\n",
        "    window = melody_notes[i:i + window_size]\n",
        "    print(type(window), window)  # Debugging line: Check window content\n",
        "    model_input = np.expand_dims(to_one_hot(window, num_unique_notes), axis=0)  # Assuming one-hot encoded melody\n",
        "    prediction = model.predict(model_input)[0]\n",
        "    predicted_chords.append(prediction)\n",
        "\n",
        "  return predicted_chords\n",
        "\n",
        "  midi_file_path = '/content/basic_midi_file.midi'  # Adjust if necessary\n",
        "  melody_notes = extract_melody(midi_file_path)\n",
        "  num_unique_notes = 12  # Assuming a single octave\n"
      ],
      "metadata": {
        "id": "DV7evVJcaqe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code snippet provided is the culmination of previous steps, where a MIDI file path is specified and used to load MIDI data. The `pretty_midi` library's `PrettyMIDI` class is instantiated with the MIDI file, which then allows for the extraction of melody notes using a custom function `extract_melody`. The melody is then fed into a model that suggests chords to accompany the melody, using the `suggest_chords_for_melody` function.\n",
        "\n",
        "Here's a detailed explanation of the process:\n",
        "\n",
        "1. I set the variable `midi_file_path` to the location of a MIDI file that contains the musical piece I'm working with.\n",
        "\n",
        "2. Using the `pretty_midi` library, I create a `PrettyMIDI` object with this file. This object allows me to interact with the musical data contained within the MIDI file.\n",
        "\n",
        "3. I call the `extract_melody` function on the `PrettyMIDI` object to isolate the melody from the rest of the musical information. This function is designed to identify and separate the melody based on certain musical properties, such as pitch or the instrument's track.\n",
        "\n",
        "4. With the melody notes in hand, I use the pre-trained neural network model within the `suggest_chords_for_melody` function. This model has been trained to predict chords that harmonize with given melodies.\n",
        "\n",
        "5. The `suggest_chords_for_melody` function processes the melody and produces a list of suggested chords that would likely harmonize with each segment of the melody.\n",
        "\n",
        "6. Finally, I print out the list of suggested chords. This output serves as a guide for creating a harmonized version of the original melody and can be used by musicians for arrangement purposes or further music production work."
      ],
      "metadata": {
        "id": "kqGH-nLb2BQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "midi_file_path = '/content/basic_midi_file.midi'\n",
        "midi_data = pretty_midi.PrettyMIDI(midi_file_path)\n",
        "melody_notes = extract_melody(midi_data)\n",
        "suggested_chords = suggest_chords_for_melody(model, melody_notes)\n",
        "print(\"Suggested Chords:\", suggested_chords)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4kNJ6PgBjQFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In conclusion, despite the unexpected setback of not being able to generate chords due to some elusive errors, this assignment was a meaningful journey through the intricacies of music generation with AI. Notably, significant accomplishments were made:\n",
        "\n",
        "1. Successfully installed and utilized key libraries such as `pretty_midi`, setting the foundation for MIDI data manipulation.\n",
        "2. Gained hands-on experience in processing MIDI files, extracting valuable musical information.\n",
        "3. Developed a deeper understanding of neural network architectures by constructing a model aimed at music generation.\n",
        "4. Applied concepts of data preprocessing, normalization, and preparation for model compatibility.\n",
        "5. Strengthened problem-solving skills, particularly in debugging and iterative code improvement.\n",
        "\n",
        "While the final goal of chord generation remains unmet, the skills and knowledge acquired lay the groundwork for continued exploration and development in this area. Moving forward, I will persist in debugging the current issues, with a firm belief that a solution is within reach. This perseverance, coupled with the progress made thus far, assures me that success is not a matter of 'if' but 'when'."
      ],
      "metadata": {
        "id": "XDK82Ok33OzK"
      }
    }
  ]
}